{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "001596af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m126 packages\u001b[0m \u001b[2min 16ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m104 packages\u001b[0m \u001b[2min 30ms\u001b[0m\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m126 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m104 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m126 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m104 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add langchain\n",
    "!uv add langchain-openai\n",
    "!uv add langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb9c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_4a331a0222', 'id': 'chatcmpl-Cf0LOKvPd5nlWxMpL6o7OMenGxTmv', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--16d75d8d-8c97-4ed3-a5d0-4debbf7f97ec-0', usage_metadata={'input_tokens': 14, 'output_tokens': 8, 'total_tokens': 22, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7,\n",
    "    base_url='https://api.apiyi.com/v1'\n",
    ")\n",
    "\n",
    "llm.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2d0f063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 35, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_4a331a0222', 'id': 'chatcmpl-Cf0LcrG0ZFK71DnYYFtAmUyKb3Sch', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--a15c6794-a44f-4616-9ffb-c338bcca5598-0', usage_metadata={'input_tokens': 35, 'output_tokens': 8, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªåŒ…å«input_promptå˜é‡çš„æç¤ºè¯æ¨¡æ¿\n",
    "template = \"\"\"<s><|user|>\n",
    "{input_prompt}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input_prompt\"]\n",
    ")\n",
    "\n",
    "basic_chain = prompt | llm\n",
    "\n",
    "# ä½¿ç”¨é“¾å¼æ¶æ„\n",
    "basic_chain.invoke({\"input_prompt\": \"My name is Maarten, What is the capital of France?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff85f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# å®šä¹‰ Prompt\n",
    "# æç¤ºï¼šä½ çš„ template ä¸­åŒ…å«äº†ç‰¹æ®Š tokens (<s><|user|>...)ï¼Œé€šå¸¸ç”¨äºæœ¬åœ°æ¨¡å‹æˆ–åŸå§‹è¡¥å…¨æ¨¡å‹ã€‚\n",
    "template = \"\"\"<s><|user|>\n",
    "Create a title for a story about {summary}. Only return the title.<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n",
    "\n",
    "# å®šä¹‰è¾“å‡ºè§£æå™¨\n",
    "# ç›´æ¥å°† LLM çš„è¾“å‡ºå¯¹è±¡ (AIMessage) è½¬ä¸ºçº¯å­—ç¬¦ä¸²ï¼Œç›¸å½“äºä»¥å‰çš„ output_key=\"title\"\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# æ„å»ºé“¾ (ä½¿ç”¨ç®¡é“ç¬¦ | )\n",
    "# æµç¨‹ï¼šè¾“å…¥ -> æç¤ºè¯ -> æ¨¡å‹ -> å­—ç¬¦ä¸²è§£æ\n",
    "chain = title_prompt | llm | output_parser\n",
    "\n",
    "# è°ƒç”¨é“¾ (ä½¿ç”¨ invoke)\n",
    "summary_text = \"a girl that find her boyfriend\"\n",
    "title = chain.invoke({\"summary\": summary_text})\n",
    "\n",
    "print(title)\n",
    "print(\"------------end title-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40704efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è§’è‰²æè¿°\n",
    "# ä½¿ç”¨æ•…äº‹æ¢—æ¦‚å’Œæ ‡é¢˜åˆ›å»ºä¸€ä¸ªé“¾å¼æ¶æ„æ¥ç”Ÿæˆè§’è‰²æè¿°\n",
    "template = \"\"\"<s><|user|>\n",
    "Describe the main character of a story about {summary} with the title {title}.\n",
    "Use only two sentences.<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "character_prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"summary\", \"title\"]\n",
    ")\n",
    "chain = character_prompt | llm | output_parser\n",
    "character_text = \"a brave knight who specifically fights data bugs\"\n",
    "character = chain.invoke({\"summary\": character_text, \"title\": title})\n",
    "print(character)\n",
    "print(\"------------end character-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab03f8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨æ•…äº‹æ¢—æ¦‚ã€æ ‡é¢˜å’Œè§’è‰²æè¿°åˆ›å»ºä¸€ä¸ªé“¾å¼æ¶æ„æ¥ç”Ÿæˆæ•…äº‹\n",
    "template = \"\"\"<s><|user|>\n",
    "Create a story about {summary} with the title {title}. The main character is:\n",
    "{character}. Only return the story and it cannot be longer than one paragraph.\n",
    "<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "story_prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"summary\", \"title\", \"character\"]\n",
    ")\n",
    "\n",
    "chain = story_prompt | llm | output_parser\n",
    "summary_text = \"a girl that find her favorite music\"\n",
    "story = chain.invoke({\"summary\": character_text, \"title\": title, \"character\": character})\n",
    "print(story)\n",
    "print(\"------------end story-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dcb22e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm sorry, but I don't have access to personal data about individuals unless it is shared with me during our conversation. If you tell me your name, I will be happy to address you accordingly.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 27, 'total_tokens': 67, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_4a331a0222', 'id': 'chatcmpl-Cf0NX7ivIhFuNUpHOT526WfgpG2yY', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--253a5ac8-9de3-4fa7-ace1-60d856fdbf7b-0', usage_metadata={'input_tokens': 27, 'output_tokens': 40, 'total_tokens': 67, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å‘Šè¯‰LLMæˆ‘ä»¬çš„åå­—\n",
    "basic_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})\n",
    "\n",
    "# æ¥ä¸‹æ¥ï¼Œè®©LLMé‡å¤è¿™ä¸ªåå­—\n",
    "basic_chain.invoke({\"input_prompt\": \"What is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba32a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Hello Maarten! The answer to 1 + 1 is 2.\n",
      "--------------------\n",
      "AI: Your name is Maarten.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "\n",
    "\n",
    "# å®šä¹‰ Prompt\n",
    "# æ³¨æ„ï¼šå¯¹äº ChatModel (å¦‚ GPT-4o)ï¼Œæ¨èä½¿ç”¨ ChatPromptTemplate + MessagesPlaceholder\n",
    "# è¿™æ¯”æ‰‹åŠ¨å†™ \"<s><|user|>\" æ›´ç¬¦åˆæ¨¡å‹åŸç”Ÿç»“æ„ï¼Œæ•ˆæœæ›´å¥½ã€‚\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    # è¿™é‡Œçš„ MessagesPlaceholder ä¼šè‡ªåŠ¨æŠŠå†å²æ¶ˆæ¯åˆ—è¡¨å¡«å……è¿›æ¥\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "    (\"human\", \"{input_prompt}\"),\n",
    "])\n",
    "\n",
    "# æ„å»ºåŸºç¡€é“¾ (LCEL)\n",
    "# æ­¤æ—¶è¿™ä¸ªé“¾è¿˜ä¸çŸ¥é“æ€ä¹ˆå¤„ç†è®°å¿†ï¼Œå®ƒåªæ˜¯å•çº¯çš„ï¼šè¾“å…¥ -> æç¤ºè¯ -> æ¨¡å‹ -> å­—ç¬¦ä¸²\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# å®šä¹‰è®°å¿†å­˜å‚¨ç®¡ç†\n",
    "# æˆ‘ä»¬éœ€è¦ä¸€ä¸ªåœ°æ–¹æ¥å­˜ä¸åŒ session_id çš„èŠå¤©è®°å½•\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# åŒ…è£…é“¾ï¼šæ·»åŠ è®°å¿†åŠŸèƒ½\n",
    "# RunnableWithMessageHistory ä¼šè‡ªåŠ¨å¤„ç†ï¼š\n",
    "# 1. è¿è¡Œå‰è¯»å– history æ³¨å…¥åˆ° prompt çš„ chat_history å˜é‡\n",
    "# 2. è¿è¡Œåå°† input_prompt å’Œè¾“å‡ºç»“æœä¿å­˜å› history\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input_prompt\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "# è°ƒç”¨ (å¿…é¡»æä¾› config åŒ…å« session_id)\n",
    "# ç¬¬ä¸€è½®å¯¹è¯\n",
    "response1 = chain_with_history.invoke(\n",
    "    {\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user_123\"}}\n",
    ")\n",
    "print(f\"AI: {response1}\")\n",
    "\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# ç¬¬äºŒè½®å¯¹è¯ (æµ‹è¯•è®°å¿†)\n",
    "response2 = chain_with_history.invoke(\n",
    "    {\"input_prompt\": \"What is my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user_123\"}} # ä½¿ç”¨ç›¸åŒçš„ session_id\n",
    ")\n",
    "print(f\"AI: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0c395de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Round 1 ---\n",
      "Hi Maarten! It's great to meet you. How can I assist you today?\n",
      "\n",
      "--- Round 2 ---\n",
      "1 + 1 equals 2! ğŸ˜Š\n",
      "\n",
      "--- Round 3 ---\n",
      "Your name is Maarten! ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "\n",
    "# 1. å®šä¹‰ LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7,\n",
    "    base_url='https://api.apiyi.com/v1'\n",
    ")\n",
    "\n",
    "# 2. å®šä¹‰ Prompt\n",
    "# ä½¿ç”¨ MessagesPlaceholder æ¥æ¥æ”¶å†å²è®°å½•\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "    (\"human\", \"{input_prompt}\"),\n",
    "])\n",
    "\n",
    "# 3. å®šä¹‰ Window é€»è¾‘ (å…³é”®æ­¥éª¤)\n",
    "# k=2 æ„å‘³ç€ä¿ç•™æœ€å 2 è½®å¯¹è¯ = 4 æ¡æ¶ˆæ¯ (User, AI, User, AI)\n",
    "def filter_k_history(input_dict):\n",
    "    history = input_dict.get(\"chat_history\", [])\n",
    "    # å¦‚æœå†å²è®°å½•è¶…è¿‡ 4 æ¡ï¼Œåªå–æœ€å 4 æ¡\n",
    "    return history[-4:] if len(history) > 4 else history\n",
    "\n",
    "# 4. æ„å»ºé“¾ (LCEL)\n",
    "# æµç¨‹: \n",
    "# (a) RunnableWithMessageHistory æ³¨å…¥å…¨é‡ chat_history\n",
    "# (b) assign æ­¥éª¤è°ƒç”¨ filter_k_history æˆªæ–­ chat_history\n",
    "# (c) ä¼ ç»™ Prompt -> LLM -> Parser\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(chat_history=filter_k_history) \n",
    "    | prompt \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 5. è®°å¿†å­˜å‚¨ç®¡ç†\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# 6. åŒ…è£…é“¾\n",
    "chain_with_window_memory = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input_prompt\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "# --- æµ‹è¯•éªŒè¯ ---\n",
    "\n",
    "session_config = {\"configurable\": {\"session_id\": \"window_test_user\"}}\n",
    "\n",
    "# ç¬¬ 1 è½®\n",
    "print(\"--- Round 1 ---\")\n",
    "response = chain_with_window_memory.invoke(\n",
    "    {\"input_prompt\": \"Hi! My name is Maarten and I am 33 years old.\"}, \n",
    "    config=session_config\n",
    ")\n",
    "print(f\"{response}\")\n",
    "\n",
    "# ç¬¬ 2 è½®\n",
    "print(\"\\n--- Round 2 ---\")\n",
    "response = chain_with_window_memory.invoke(\n",
    "    {\"input_prompt\": \"What is 1 + 1?\"}, \n",
    "    config=session_config\n",
    ")\n",
    "print(f\"{response}\")\n",
    "\n",
    "# ç¬¬ 3 è½® (æ­¤æ—¶ç¬¬ 1 è½®çš„å¯¹è¯åº”è¯¥è¢«æŒ¤å‡ºçª—å£)\n",
    "print(\"\\n--- Round 3 ---\")\n",
    "# æ­¤æ—¶å†å²è®°å½•é‡Œæœ‰ Round 1 (User+AI) å’Œ Round 2 (User+AI)ï¼Œå…± 4 æ¡ã€‚\n",
    "# å†æ¬¡æé—®æ—¶ï¼ŒRound 1 å°†è¢«åˆ‡é™¤ï¼ŒRound 2 å’Œæœ¬æ¬¡ Round 3 çš„è¾“å…¥å°†æ„æˆæ–°çš„ä¸Šä¸‹æ–‡ã€‚\n",
    "response = chain_with_window_memory.invoke(\n",
    "    {\"input_prompt\": \"What is my name?\"}, \n",
    "    config=session_config\n",
    ")\n",
    "\n",
    "print(f\"{response}\")\n",
    "# é¢„æœŸå›ç­”ï¼šAI å¯èƒ½ä¼šè¯´ä¸çŸ¥é“ï¼Œæˆ–è€…åªè®°å¾—ä¸Šä¸‹æ–‡é‡Œçš„ä¿¡æ¯ï¼Œå› ä¸ºåŒ…å«åå­—çš„é‚£å¥è¯å·²ç»è¢«æˆªæ–­äº†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ddd3edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Round 4 ---\n",
      "I don't know your age unless you tell me! If you'd like to share some details, I'm happy to assist you further. ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# æ£€æŸ¥æ¨¡å‹æ˜¯å¦è®°å¾—æˆ‘ä»¬ç»™å‡ºçš„åå­—\n",
    "print(\"\\n--- Round 4 ---\")\n",
    "response = chain_with_window_memory.invoke(\n",
    "    {\"input_prompt\":\"What is my age?\"},\n",
    "    config=session_config\n",
    ")\n",
    "print(f\"{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd39292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/79/f9qbnfls3r51cr3tsymr2j0m0000gn/T/ipykernel_90082/1867450804.py:27: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Round 1 ---\n",
      "AI: Hello Maarten! 1 + 1 equals 2. ğŸ˜Š How can I assist you further?\n",
      "\n",
      "--- Round 2 ---\n",
      "AI: Your name is Maarten!\n",
      "\n",
      "--- Round 3 ---\n",
      "AI: The first question you asked was, \"What is 1 + 1?\"\n",
      "\n",
      "--- Memory Content ---\n",
      "{'summary': 'Maarten introduces himself and asks a simple math question, \"What is 1 + 1?\" The AI responds by greeting Maarten, providing the correct answer (2), and offering further assistance. Maarten then asks, \"What is my name?\" and the AI correctly identifies his name as Maarten. Maarten follows up by asking, \"What was the first question I asked?\" and the AI accurately recalls that the first question was, \"What is 1 + 1?\"'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.memory import ConversationSummaryMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 1. å®šä¹‰æ‘˜è¦çš„ Prompt (ä¿æŒä½ çš„åŸå§‹æ¨¡æ¿)\n",
    "summary_prompt_template = \"\"\"<s><|user|>Summarize the conversations and update\n",
    "with the new lines.\n",
    "Current summary:\n",
    "{summary}\n",
    "new lines of conversation:\n",
    "{new_lines}\n",
    "New summary:<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"new_lines\", \"summary\"],\n",
    "    template=summary_prompt_template\n",
    ")\n",
    "\n",
    "# 2. åˆå§‹åŒ–è®°å¿†ç»„ä»¶\n",
    "# æ³¨æ„ï¼šConversationSummaryMemory ä¾ç„¶å¯ä»¥ä½¿ç”¨ï¼Œä½†åœ¨ LCEL ä¸­æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨è°ƒç”¨å®ƒçš„ load å’Œ save\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    "    memory_key=\"summary\", # è¿™é‡Œæˆ‘ä»¬å°† key è®¾ä¸º summaryï¼Œä»¥ä¾¿åœ¨ä¸» prompt ä¸­ä½¿ç”¨\n",
    "    prompt=summary_prompt\n",
    ")\n",
    "\n",
    "# 3. å®šä¹‰ä¸»å¯¹è¯çš„ Prompt\n",
    "# æ³¨æ„ï¼šåœ¨ä½¿ç”¨ LCEL æ—¶ï¼ŒPrompt å¿…é¡»æ˜¾å¼åŒ…å« summary çš„å ä½ç¬¦\n",
    "main_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"ä»¥ä¸‹æ˜¯ç›®å‰çš„å¯¹è¯æ‘˜è¦:\n",
    "{summary}\n",
    "\n",
    "ç”¨æˆ·: {input_prompt}\n",
    "AI:\"\"\"\n",
    ")\n",
    "\n",
    "# 4. æ„å»º LCEL é“¾\n",
    "# é€»è¾‘ï¼šå…ˆè·å– input_prompt -> ä» memory åŠ è½½ summary -> ä¼ ç»™ prompt -> llm -> è§£æè¾“å‡º\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        summary=lambda x: memory.load_memory_variables({})[\"summary\"]\n",
    "    )\n",
    "    | main_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 5. å®šä¹‰ä¸€ä¸ªæ‰§è¡Œå‡½æ•°æ¥å¤„ç†â€œè°ƒç”¨â€å’Œâ€œä¿å­˜è®°å¿†â€\n",
    "def chat_with_memory(input_text):\n",
    "    # 1. è°ƒç”¨é“¾ç”Ÿæˆå›å¤\n",
    "    response = chain.invoke({\"input_prompt\": input_text})\n",
    "    \n",
    "    # 2. æ‰‹åŠ¨ä¿å­˜ä¸Šä¸‹æ–‡åˆ°è®°å¿†ä¸­ (è¿™æ˜¯ LCEL ä¸æ—§ç‰ˆ LLMChain æœ€å¤§çš„ä¸åŒ)\n",
    "    memory.save_context(\n",
    "        {\"input\": input_text}, \n",
    "        {\"output\": response}\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# --- æµ‹è¯•ä»£ç  ---\n",
    "\n",
    "# ç¬¬ä¸€è½®å¯¹è¯\n",
    "print(\"--- Round 1 ---\")\n",
    "response1 = chat_with_memory(\"Hi! My name is Maarten. What is 1 + 1?\")\n",
    "print(f\"AI: {response1}\")\n",
    "\n",
    "# ç¬¬äºŒè½®å¯¹è¯\n",
    "print(\"\\n--- Round 2 ---\")\n",
    "response2 = chat_with_memory(\"What is my name?\")\n",
    "print(f\"AI: {response2}\")\n",
    "\n",
    "# ç¬¬ä¸‰è½®å¯¹è¯ï¼šè¯¢é—®ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯ä»€ä¹ˆï¼ˆè¿™å–å†³äºæ‘˜è¦æ˜¯å¦åŒ…å«è¯¥ç»†èŠ‚ï¼‰\n",
    "print(\"\\n--- Round 3 ---\")\n",
    "response3 = chat_with_memory(\"What was the first question I asked?\")\n",
    "print(f\"AI: {response3}\")\n",
    "\n",
    "# æŸ¥çœ‹å½“å‰çš„æ‘˜è¦çŠ¶æ€\n",
    "print(\"\\n--- Memory Content ---\")\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa981880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºReActæ¨¡æ¿\n",
    "react_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "{tools}\n",
    "Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "Begin!\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=react_template,\n",
    "    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5890f039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m136 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m113 packages\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m137 packages\u001b[0m \u001b[2min 438ms\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/0)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)-------------------\u001b[0m\u001b[0m     0 B/148.52 KiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)-------------------\u001b[0m\u001b[0m 16.00 KiB/148.52 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)-------------------\u001b[0m\u001b[0m 32.00 KiB/148.52 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)-------------------\u001b[0m\u001b[0m 48.00 KiB/148.52 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)m------------------\u001b[0m\u001b[0m 64.00 KiB/148.52 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)m\u001b[2m--------------\u001b[0m\u001b[0m 80.00 KiB/148.52 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)[30m\u001b[2m-----------\u001b[0m\u001b[0m 96.00 KiB/148.52 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--\u001b[30m\u001b[2m--------\u001b[0m\u001b[0m 112.00 KiB/148.52 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--\u001b[30m\u001b[2m--------\u001b[0m\u001b[0m 112.00 KiB/148.52 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)-----\u001b[30m\u001b[2m-----\u001b[0m\u001b[0m 128.00 KiB/148.52 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)---------\u001b[30m\u001b[2m-\u001b[0m\u001b[0m 144.00 KiB/148.52 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)----------\u001b[30m\u001b[2m\u001b[0m\u001b[0m 148.52 KiB/148.52 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 205ms\u001b[0m\u001b[0m                                                       \u001b[1A\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m                                  \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumexpr\u001b[0m\u001b[2m==2.14.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add ddgs\n",
    "!uv add numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb9cca4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·¥å…·å‡†å¤‡å®Œæ¯•ï¼Œå¯ç”¨äº Agent æ„é€ ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/79/f9qbnfls3r51cr3tsymr2j0m0000gn/T/ipykernel_90082/1069184275.py:18: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  math_chain = LLMMathChain(llm=llm, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import Tool\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain_classic.chains import LLMMathChain\n",
    "\n",
    "# --- 1. åˆ›å»º DuckDuckGo æœç´¢å·¥å…· ---\n",
    "# DuckDuckGoSearchResults æ¨¡å—è¢«ç§»åˆ°äº† langchain_community\n",
    "search = DuckDuckGoSearchResults()\n",
    "\n",
    "# Tool åŒ…è£…å™¨è¢«ç§»åˆ°äº† langchain_core.tools\n",
    "search_tool = Tool(\n",
    "    name=\"duckduck\",\n",
    "    description=\"A web search engine. Use this to as a search engine for general queries.\",\n",
    "    func=search.run, # .run() æ–¹æ³•ä»ç„¶æ˜¯æ‰§è¡Œå·¥å…·çš„æ ‡å‡†æ¥å£\n",
    ")\n",
    "\n",
    "# --- 2. åˆ›å»º LLM Math å·¥å…· (å–ä»£ load_tools([\"llm-math\"], llm=llm)) ---\n",
    "# åœ¨æ–° API ä¸­ï¼Œéœ€è¦ç›´æ¥å®ä¾‹åŒ– LLMMathChain\n",
    "math_chain = LLMMathChain(llm=llm, verbose=False)\n",
    "\n",
    "math_tool = Tool(\n",
    "    name=\"Calculator\", # å»ºè®®ä¸ºå·¥å…·èµ·ä¸€ä¸ªæè¿°æ€§çš„åç§°\n",
    "    description=\"Useful for when you need to answer questions about math.\",\n",
    "    func=math_chain.run, # å°† LLMMathChain çš„ run æ–¹æ³•ä½œä¸ºå·¥å…·å‡½æ•°\n",
    ")\n",
    "\n",
    "\n",
    "# --- 3. å‡†å¤‡å·¥å…·åˆ—è¡¨ ---\n",
    "tools = [math_tool, search_tool]\n",
    "\n",
    "print(\"âœ… å·¥å…·å‡†å¤‡å®Œæ¯•ï¼Œå¯ç”¨äº Agent æ„é€ ã€‚\")\n",
    "# print(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0055c035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Agent Executor (å¯è¿è¡Œçš„æ™ºèƒ½ä½“) æ„é€ å®Œæˆã€‚\n"
     ]
    }
   ],
   "source": [
    "# ç¡®ä¿å¯¼å…¥è·¯å¾„æ˜¯æ­£ç¡®çš„ (æ¥è‡ª langchain å’Œ langchain_core)\n",
    "from langchain_classic.agents import create_react_agent, AgentExecutor\n",
    "\n",
    "# å‡è®¾ llm å’Œ tools å·²åœ¨å‰é¢çš„æ­¥éª¤ä¸­æ­£ç¡®åˆå§‹åŒ–å’Œå®šä¹‰ã€‚\n",
    "\n",
    "# --- 1. å®šä¹‰ç°ä»£ ReAct æç¤ºè¯ç»“æ„ ---\n",
    "# åˆ›å»º ReAct æ¨¡æ¿\n",
    "react_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "{tools}\n",
    "Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "Begin!\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=react_template,\n",
    "    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",
    ")\n",
    "\n",
    "# --- 2. æ„å»º ReAct æ™ºèƒ½ä½“é€»è¾‘ (Agent Logic) ---\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "\n",
    "# --- 3. æ„å»ºæ™ºèƒ½ä½“æ‰§è¡Œå™¨ (Agent Executor) ---\n",
    "# AgentExecutor ä¾ç„¶ç”¨äºåŒ…è£…é€»è¾‘å’Œå·¥å…·ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªå¯è¿è¡Œçš„ (Runnable) å¯¹è±¡\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "print(\"âœ… Agent Executor (å¯è¿è¡Œçš„æ™ºèƒ½ä½“) æ„é€ å®Œæˆã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a64feea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo answer this question, I need to find the current price of a MacBook Pro in USD and then convert that price to EUR using the given exchange rate. First, I'll look up the current price of a MacBook Pro in USD.\n",
      "\n",
      "Action: duckduck  \n",
      "Action Input: \"current price of MacBook Pro in USD October 2023\"  \u001b[0m\u001b[33;1m\u001b[1;3msnippet: Amazon has a fresh batch of all-time low prices on Apple's 16-inch MacBook Pro today, starting at $1,999.99 for the 10-Core M1 Pro /512GB model, down ..., title: Deals: New $499 Discounts Hit Apple's 16-Inch MacBook Pro on, link: https://www.macrumors.com/2022/11/16/deals-499-discounts-16-inch-macbook-pro/, snippet: ... time is about 24 seconds which is longer than I expected, but with one of the MacBook Pro â€™ s best new features, Touch ID, getting back into ..., title: 2016 macbook pro Archives - My Tech Methods, link: https://mytechmethods.com/tag/2016-macbook-pro/, snippet: ... time is about 24 seconds which is longer than I expected, but with one of the MacBook Pro â€™ s best new features, Touch ID, getting back into ..., title: macbook pro 2016 Archives - My Tech Methods, link: https://mytechmethods.com/tag/macbook-pro-2016/, snippet: Certain details, including but not limited to prices and special offers, are provided to us directly from our partners and are dynamic and subject to ..., title: New MacBook Pro: Everything We Need to Know, link: https://www.bestmactools.com/new-macbook-pro/\u001b[0m\u001b[32;1m\u001b[1;3mThe search results did not provide a clear answer to the current price of a MacBook Pro. I will refine the search query for more accurate information.\n",
      "\n",
      "Action: duckduck  \n",
      "Action Input: \"current cost of Apple MacBook Pro October 2023\"  \u001b[0m\u001b[33;1m\u001b[1;3msnippet: currentï¼Œpresentï¼Œrecentåœ¨è¡¨ç¤ºå½¢å®¹è¯æ€§çš„æ—¶å€™å‘ç”Ÿçš„æ—¶é—´ä¸åŒã€è¯æ€§ä¸åŒã€åœ¨å¥å‹ä¸­ç”¨æ³•ä¸åŒã€‚ ä¸€ã€è¡¨ç¤ºå½¢å®¹è¯æ€§çš„æ—¶å€™ï¼Œè¡¨ç¤ºå‘ç”Ÿçš„æ—¶é—´ä¸åŒ 1ã€currentæŒ‡ç›®å‰å­˜åœ¨å’Œå‘ç”Ÿçš„ã€‚ 2ã€recentè¡¨ç¤ºæ–°è¿‘ â€¦, title: currentï¼Œpresentï¼Œrecentä¸‰ä¸ªè¯æœ‰ä»€ä¹ˆåŒºåˆ« - ç™¾åº¦çŸ¥é“, link: https://zhidao.baidu.com/question/434865041797516004.html, snippet: Feb 17, 2007 Â· I don't think current and present are generally interchangeable. Current has more of an idea of what it happening at the present time in general. The current theory is that the â€¦, title: current and present - WordReference Forums, link: https://forum.wordreference.com/threads/current-and-present.391667/, snippet: Jul 14, 2015 Â· I presume that \"then-current\" is sometimes need to avoid ambiguity (because \"current\" alone can mean \"at the present time\"). If I said \"Shakespeare used the current phrase â€¦, title: current / then-current/ then (as an adjective to mean \"at that time ..., link: https://forum.wordreference.com/threads/current-then-current-then-as-an-adjective-to-mean-at-that-time.3043755/, snippet: Oct 24, 2014 Â· Hi everyone. What does this phrase in the snapshot mean? \"No, Your Honor. I'm current on all the rules.\" [Relevant phrase copied to post, DonnyB - moderator], title: Be current on Something - WordReference Forums, link: https://forum.wordreference.com/threads/be-current-on-something.3874390/\u001b[0m\u001b[32;1m\u001b[1;3mIt seems the search results did not provide relevant information about the current price of a MacBook Pro. I will refine the search further to get accurate results.\n",
      "\n",
      "Action: duckduck  \n",
      "Action Input: \"MacBook Pro price October 2023 in USD\"  \u001b[0m\u001b[33;1m\u001b[1;3msnippet: Technical Details. Apple 2023 MacBook Pro 14-inch with Touch ID., title: Amazon.com: Apple 2023 MacBook Pro Laptop with Apple M2 Pro ..., link: https://www.amazon.com/Apple-MacBook-Laptop-10â€‘core-16â€‘core/dp/B0BSHDVBXZ, snippet: The wait for the MacBook Pro with the M5 Pro and M5 Max chips could extend to spring 2026. The new M5 MacBook Pro models can be ordered now and will start to ship from October 22. Hereâ€™s the lowdown on the specs, price , new features, design, dimensions and more., title: New M5 MacBook Pro : Release date, price , specs and... | Macworld, link: https://www.macworld.com/article/2552098/m5-macbook-pro-design-processor-specs-release.html, snippet: Tips / Contact Us . Report: Low-Cost iPhone, iPad, MacBook Coming Early 2026. Friday November 21, 2025 2:26 am PST by Tim Hardwick. Apple is planning to release new entry-level models across its iPhone, iPad, and Mac categories early next year, with an..., title: Report: Low-Cost iPhone, iPad, MacBook Coming Early... - MacRumors, link: https://www.macrumors.com/2025/11/21/low-cost-iphone-ipad-macbook-early-2026/, snippet: Last Updated: October 25, 2023 , 16:25 IST.The image, which was re- shared on X by the leaker known as ShrimpApplePro, shows what appears to be Apple packaging displaying an image of the MacBook Pro with a previously unreleased wallpaper on the display., title: New MacBook Pro Box Image Surfaces Online Ahead Of October 30..., link: https://www.news18.com/tech/new-macbook-pro-box-image-surfaces-online-ahead-of-october-30-launch-8633160.html\u001b[0m\u001b[32;1m\u001b[1;3mThe search results are still not providing a clear answer regarding the current price of a MacBook Pro in October 2023. Instead, I will provide an approximate price based on known recent models and trends. Typically, the base model of the MacBook Pro starts around $1,299 for the 13-inch variant, while the more advanced models (14-inch and 16-inch) may range between $1,999 and $2,499 depending on specifications.\n",
      "\n",
      "Using $1,999 as a representative price, I will now calculate the cost in EUR with the given exchange rate of 0.85 EUR/USD.\n",
      "\n",
      "Thought: Convert $1,999 to EUR using the exchange rate.\n",
      "\n",
      "Action: Calculator  \n",
      "Action Input: 1999 * 0.85  \u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 1699.1499999999999\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "\n",
      "Final Answer: The current price of a MacBook Pro is approximately $1,999 USD. At an exchange rate of 0.85 EUR/USD, it would cost approximately â‚¬1,699.15 EUR.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD.',\n",
       " 'output': 'The current price of a MacBook Pro is approximately $1,999 USD. At an exchange rate of 0.85 EUR/USD, it would cost approximately â‚¬1,699.15 EUR.'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MacBook Proçš„ä»·æ ¼æ˜¯å¤šå°‘ï¼Ÿ\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD.\"\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hands-on-large-language-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
