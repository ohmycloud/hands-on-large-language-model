{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYv9r8YCkJrT"
      },
      "outputs": [],
      "source": [
        "!curl -LsSf https://astral.sh/uv/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJG9joKioJ3k"
      },
      "outputs": [],
      "source": [
        "!mkdir hands_on_genai\n",
        "!cd hands_on_genai && uv init && uv add transformers && uv add sentence_transformers && uv add gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2xGVmt4oTag"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# 加载分词器\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
        "\n",
        "# 加载语言模型\n",
        "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
        "\n",
        "#对句子进行分词\n",
        "tokens = tokenizer(\"Hello World\", return_tensors='pt')\n",
        "\n",
        "# 处理词元\n",
        "output = model(**tokens)[0]\n",
        "\n",
        "print(output.shape)\n",
        "for token in tokens['input_ids'][0]:\n",
        "    print(tokenizer.decode(token))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WVcMp8iEp8A8"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 加载模型\n",
        "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "# 将文本转换为文本嵌入\n",
        "vector = model.encode(\"Best movie ever!\")\n",
        "\n",
        "print(vector.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05JMbUZbCVVE"
      },
      "outputs": [],
      "source": [
        "!pip install gensim\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLB9pbpZ9boD"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# glove, 训练数据来自维基百科\n",
        "# 其它选项包括 word2vec-google-news-300\n",
        "model = api.load(\"glove-wiki-gigaword-50\")\n",
        "model.most_similar([model['king']], topn=11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsRMSfA_EvW5"
      },
      "outputs": [],
      "source": [
        "# 歌曲推荐 Embedding\n",
        "import pandas as pd\n",
        "from urllib import request\n",
        "\n",
        "# 获取播放列表数据集文件\n",
        "data = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/train.txt')\n",
        "\n",
        "# 解析播放列表数据集文件。跳过前两行，因为它们只包含元数据\n",
        "lines = data.read().decode('utf-8').split('\\n')[2:]\n",
        "playlists = [s.rstrip().split() for s in lines if len(s.split()) > 1]\n",
        "\n",
        "# 加载歌曲元数据\n",
        "songs_file = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/song_hash.txt')\n",
        "songs_file = songs_file.read().decode('utf-8').split('\\n')\n",
        "songs = [s.rstrip().split('\\t') for s in songs_file]\n",
        "songs_df = pd.DataFrame(data=songs, columns=['id', 'title', 'artist'])\n",
        "songs_df = songs_df.set_index('id')\n",
        "\n",
        "print('Playlist #1:\\n ', playlists[0], '\\n')\n",
        "print('Playlist #2:\\n ', playlists[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12uuSbFCHsMr"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# 训练我们的 word2vec 模型\n",
        "model = Word2Vec(\n",
        "    playlists,\n",
        "    vector_size=32,\n",
        "    window=20,\n",
        "    negative=50,\n",
        "    min_count=1,\n",
        "    workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U12V77tFIC3-"
      },
      "outputs": [],
      "source": [
        "song_id = 2172\n",
        "print(songs_df.iloc[2172])\n",
        " # 让模型找出与歌曲 2172 相似的歌曲\n",
        "model.wv.most_similar(positive=str(song_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OxQdifNIgWa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def print_recommendations(song_id):\n",
        "  similar_songs = np.array(\n",
        "      model.wv.most_similar(positive=str(song_id), topn=5)\n",
        "  )[:, 0]\n",
        "\n",
        "  return songs_df.iloc[similar_songs]\n",
        "\n",
        "print_recommendations(2172)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
